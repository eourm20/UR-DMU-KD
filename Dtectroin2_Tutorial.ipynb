{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.1 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (5.1)\n",
      "Cloning into 'detectron2'...\n",
      "remote: Enumerating objects: 15681, done.\u001b[K\n",
      "remote: Counting objects: 100% (404/404), done.\u001b[K\n",
      "remote: Compressing objects: 100% (310/310), done.\u001b[K\n",
      "remote: Total 15681 (delta 180), reused 274 (delta 94), pack-reused 15277\u001b[K\n",
      "Receiving objects: 100% (15681/15681), 6.50 MiB | 2.08 MiB/s, done.\n",
      "Resolving deltas: 100% (11292/11292), done.\n",
      "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
      "Requirement already satisfied: Pillow>=7.1 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (3.7.5)\n",
      "Collecting pycocotools>=2.0.2\n",
      "  Using cached pycocotools-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting termcolor>=1.1\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting yacs>=0.1.8\n",
      "  Using cached yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: cloudpickle in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (3.0.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (4.66.2)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting fvcore<0.1.6,>=0.1.5\n",
      "  Using cached fvcore-0.1.5.post20221221-py3-none-any.whl\n",
      "Collecting iopath<0.1.10,>=0.1.7\n",
      "  Using cached iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Collecting omegaconf<2.4,>=2.1\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting hydra-core>=1.1\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting black\n",
      "  Using cached black-24.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: packaging in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (24.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from matplotlib) (6.3.1)\n",
      "Requirement already satisfied: PyYAML in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from yacs>=0.1.8) (5.1)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Using cached grpcio-1.64.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard)\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from tensorboard) (68.2.2)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from tensorboard) (0.41.2)\n",
      "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4,>=2.1)\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: click>=8.0.0 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from black) (8.1.7)\n",
      "Collecting mypy-extensions>=0.4.3 (from black)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black)\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from black) (4.2.2)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from black) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from black) (4.9.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (7.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/subin-oh/anaconda3/envs/WVAD/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached pycocotools-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (439 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Using cached black-24.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached grpcio-1.64.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, werkzeug, termcolor, tensorboard-data-server, tabulate, pyasn1, portalocker, pathspec, omegaconf, oauthlib, mypy-extensions, grpcio, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, markdown, iopath, hydra-core, black, pycocotools, google-auth, fvcore, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-2.1.0 antlr4-python3-runtime-4.9.3 black-24.4.2 cachetools-5.3.3 fvcore-0.1.5.post20221221 google-auth-2.29.0 google-auth-oauthlib-1.0.0 grpcio-1.64.0 hydra-core-1.3.2 iopath-0.1.9 markdown-3.6 mypy-extensions-1.0.0 oauthlib-3.2.2 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.8.2 pyasn1-0.6.0 pyasn1-modules-0.4.0 pycocotools-2.0.7 requests-oauthlib-2.0.0 rsa-4.9 tabulate-0.9.0 tensorboard-2.14.0 tensorboard-data-server-0.7.2 termcolor-2.4.0 werkzeug-3.0.3 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pyyaml==5.1\n",
    "import sys, os, distutils.core\n",
    "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "# Properly install detectron2. (Please do not install twice in both ways)\n",
    "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 144\u001b[0m\n\u001b[1;32m    140\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m video_cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[0;32m--> 144\u001b[0m     keypoints, visualized_frame, one_person_detection \u001b[38;5;241m=\u001b[39m \u001b[43mpose_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pose_vi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVisualized Frame\u001b[39m\u001b[38;5;124m'\u001b[39m, visualized_frame)\n\u001b[1;32m    147\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVisualized Frame_one person\u001b[39m\u001b[38;5;124m'\u001b[39m, one_person_detection)\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mdetectron2_pose.get_pose_vi\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pose_vi\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame):\n\u001b[1;32m     29\u001b[0m     frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)  \u001b[38;5;66;03m# convert BGR image to RGB\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m     34\u001b[0m         instances \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/code/UR-DMU/detectron2/detectron2/engine/defaults.py:319\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    315\u001b[0m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m    317\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: height, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: width}\n\u001b[0;32m--> 319\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/anaconda3/envs/WVAD/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/UR-DMU/detectron2/detectron2/modeling/meta_arch/rcnn.py:150\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/code/UR-DMU/detectron2/detectron2/modeling/meta_arch/rcnn.py:213\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    211\u001b[0m         proposals \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched_inputs]\n\u001b[0;32m--> 213\u001b[0m     results, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     detected_instances \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m detected_instances]\n",
      "File \u001b[0;32m~/anaconda3/envs/WVAD/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/UR-DMU/detectron2/detectron2/modeling/roi_heads/roi_heads.py:750\u001b[0m, in \u001b[0;36mStandardROIHeads.forward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    747\u001b[0m pred_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_box(features, proposals)\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# During inference cascaded prediction is used: the mask and keypoints heads are only\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# applied to the top scoring box detections.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m pred_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_given_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_instances, {}\n",
      "File \u001b[0;32m~/code/UR-DMU/detectron2/detectron2/modeling/roi_heads/roi_heads.py:777\u001b[0m, in \u001b[0;36mStandardROIHeads.forward_with_given_boxes\u001b[0;34m(self, features, instances)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m instances[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhas(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m instances[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhas(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    776\u001b[0m instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_mask(features, instances)\n\u001b[0;32m--> 777\u001b[0m instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_keypoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instances\n",
      "File \u001b[0;32m~/code/UR-DMU/detectron2/detectron2/modeling/roi_heads/roi_heads.py:874\u001b[0m, in \u001b[0;36mStandardROIHeads._forward_keypoint\u001b[0;34m(self, features, instances)\u001b[0m\n\u001b[1;32m    872\u001b[0m     features \u001b[38;5;241m=\u001b[39m [features[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeypoint_in_features]\n\u001b[1;32m    873\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mproposal_boxes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mpred_boxes \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m instances]\n\u001b[0;32m--> 874\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeypoint_pooler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    876\u001b[0m     features \u001b[38;5;241m=\u001b[39m {f: features[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeypoint_in_features}\n",
      "File \u001b[0;32m~/anaconda3/envs/WVAD/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/UR-DMU/detectron2/detectron2/modeling/poolers.py:258\u001b[0m, in \u001b[0;36mROIPooler.forward\u001b[0;34m(self, x, box_lists)\u001b[0m\n\u001b[1;32m    255\u001b[0m output \u001b[38;5;241m=\u001b[39m _create_zeros(pooler_fmt_boxes, num_channels, output_size, output_size, x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level, pooler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevel_poolers):\n\u001b[0;32m--> 258\u001b[0m     inds \u001b[38;5;241m=\u001b[39m \u001b[43mnonzero_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_assignments\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    259\u001b[0m     pooler_fmt_boxes_level \u001b[38;5;241m=\u001b[39m pooler_fmt_boxes[inds]\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Use index_put_ instead of advance indexing, to avoid pytorch/issues/49852\u001b[39;00m\n",
      "File \u001b[0;32m~/code/UR-DMU/detectron2/detectron2/layers/wrappers.py:168\u001b[0m, in \u001b[0;36mnonzero_tuple\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os, distutils.core\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "\n",
    "\n",
    "class detectron2_pose:\n",
    "    def __init__(self):\n",
    "        # Set up the configuration and load a pre-trained model from Detectron2's model zoo\n",
    "        self.cfg = get_cfg()\n",
    "        self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "        self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "        self.predictor = DefaultPredictor(self.cfg)\n",
    "\n",
    "    def get_pose_vi(self, frame):\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # convert BGR image to RGB\n",
    "\n",
    "        outputs = self.predictor(frame_rgb)\n",
    "        \n",
    "        if \"instances\" in outputs:\n",
    "            instances = outputs[\"instances\"].to(torch.device('cpu'))\n",
    "            if instances.has(\"pred_keypoints\"):\n",
    "                keypoints_predictions = instances.pred_keypoints\n",
    "\n",
    "                # Now you can use `keypoints_predictions` tensor as needed.\n",
    "                if keypoints_predictions.shape[0] > 1:\n",
    "                    v = Visualizer(frame_rgb[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "                    v_out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "                    # cv2.imshow('Visualized Frame', v_out.get_image()[:, :, ::-1])\n",
    "                    \n",
    "                    variances = keypoints_predictions.std(dim=1).sum(dim=1)\n",
    "                    max_variance_idx = torch.argmax(variances)\n",
    "                    selectes_person = keypoints_predictions[max_variance_idx].unsqueeze(0)\n",
    "                    \n",
    "                    # tsne = TSNE(n_components=3, random_state=0)\n",
    "                    # transformed_data = tsne.fit_transform(keypoints_predictions)\n",
    "                    # print(transformed_data)\n",
    "                    # 사람이 여러명이면 하나의 의미로 합침\n",
    "                    # Get the indices of the max confidence score along the 'people' dimension (0)\n",
    "                    # conf_max_indices = keypoints_predictions[:,:,2].argmax(axis=0)\n",
    "                    # Use these indices to select the corresponding rows for each keypoint\n",
    "                    # keypoints_predictions = keypoints_predictions[conf_max_indices, torch.arange(keypoints_predictions.shape[1])]\n",
    "                    # print(\"person more than 1\")\n",
    "                    for key in instances._fields.keys():\n",
    "                        instances._fields[key] = instances._fields[key][int(max_variance_idx)]\n",
    "                    instances._fields['scores'] = instances._fields['scores'].unsqueeze(0)\n",
    "                    instances._fields['pred_classes'] = instances._fields['pred_classes'].unsqueeze(0)\n",
    "                    instances._fields['pred_keypoints'] = instances._fields['pred_keypoints'].unsqueeze(0)\n",
    "                    instances._fields['pred_keypoint_heatmaps'] = instances._fields['pred_keypoint_heatmaps'].unsqueeze(0)\n",
    "                    v2 = Visualizer(frame_rgb[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "                    v_out2 = v2.draw_instance_predictions(instances)\n",
    "                    \n",
    "                    # cv2.imshow('Visualized Frame(one person)', v_out2.get_image()[:, :, ::-1])\n",
    "                    \n",
    "                    return keypoints_predictions, v_out.get_image()[:, :, ::-1], v_out2.get_image()[:, :, ::-1]\n",
    "                else:\n",
    "                    keypoints_predictions = keypoints_predictions[0]\n",
    "                    v = Visualizer(frame_rgb[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "                    v_out = v.draw_instance_predictions(instances)\n",
    "                    \n",
    "                    return keypoints_predictions, v_out.get_image()[:, :, ::-1], v_out.get_image()[:, :, ::-1]\n",
    "                \n",
    "    def get_pose(self, frame):\n",
    "\n",
    "        # BGR 이미지를 RGB 이미지로 변환 (Matplotlib은 RGB 이미지를 사용)\n",
    "\n",
    "        # 이미지 출력\n",
    "        # plt.imshow(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        # Make prediction on the current frame and extract keypoints predictions \n",
    "        outputs = self.predictor(frame)\n",
    "        \n",
    "        if \"instances\" in outputs:\n",
    "            instances = outputs[\"instances\"].to(torch.device('cpu'))\n",
    "            if instances.has(\"pred_keypoints\"):\n",
    "                keypoints_predictions = instances.pred_keypoints\n",
    "                # print(keypoints_predictions)\n",
    "                # [x,y,신뢰도]\n",
    "                if len(keypoints_predictions)==0:\n",
    "                    keypoints_predictions = np.zeros((17,3))\n",
    "                    keypoints_predictions = keypoints_predictions.reshape(-1)\n",
    "                    return keypoints_predictions\n",
    "                else:\n",
    "                    # Now you can use `keypoints_predictions` tensor as needed.\n",
    "                    if keypoints_predictions.shape[0] > 1:\n",
    "                        tsne = TSNE(n_components=3, random_state=0)\n",
    "                        transformed_data = tsne.fit_transform(keypoints_predictions)\n",
    "                        print(transformed_data)\n",
    "                        # 사람이 여러명이면 하나의 의미로 합침\n",
    "                        # Get the indices of the max confidence score along the 'people' dimension (0)\n",
    "                        # conf_max_indices = keypoints_predictions[:,:,2].argmax(axis=0)\n",
    "                        # Use these indices to select the corresponding rows for each keypoint\n",
    "                        # keypoints_predictions = keypoints_predictions[conf_max_indices, torch.arange(keypoints_predictions.shape[1])]\n",
    "                        # print(\"person more than 1\")\n",
    "                        return keypoints_predictions\n",
    "                    else:\n",
    "                        keypoints_predictions = keypoints_predictions[0]\n",
    "                        keypoints_predictions = keypoints_predictions.reshape(-1)\n",
    "                        # print(\"person 1\")\n",
    "                        return keypoints_predictions\n",
    "                \n",
    "            \n",
    "    def normalize(self, frame):\n",
    "        # Assuming `array` is your original array\n",
    "\n",
    "        # Find the minimum and maximum of the array\n",
    "        min_val = frame.min()\n",
    "        max_val = frame.max()\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        normalized_frame = (frame - min_val) / (max_val - min_val)\n",
    "\n",
    "        # Scale to [0, 255] and convert to uint8\n",
    "        scaled_frame = (normalized_frame * 255).astype('uint8')  \n",
    "        return scaled_frame\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pose_model = detectron2_pose()\n",
    "    video_cap = cv2.VideoCapture('/home/subin-oh/Nas-subin/SB-Oh/data/Anomaly-Detection-Dataset/Train/Fighting/Fighting006_x264.mp4')\n",
    "    if not video_cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "\n",
    "    # Read until video is completed\n",
    "    while(video_cap.isOpened()):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = video_cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            \n",
    "            keypoints, visualized_frame, one_person_detection = pose_model.get_pose_vi(frame)\n",
    "        \n",
    "            cv2.imshow('Visualized Frame', visualized_frame)\n",
    "            cv2.imshow('Visualized Frame_one person', one_person_detection)\n",
    "            # Press Q on keyboard to exit (optional)\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # After reading all frames, close the display window and release video capture object\n",
    "    video_cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WVAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
